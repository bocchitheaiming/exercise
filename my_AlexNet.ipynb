{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2489523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/tmp/ipykernel_7698/1855015138.py:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_x = Variable(torch.unsqueeze(test_data.test_data, dim = 1),volatile = True).type(torch.FloatTensor)[:500]/255.\n",
      "/root/anaconda3/lib/python3.11/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "keep_prob_rate = 0.5\n",
    "max_epoch = 3\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DOWNLOAD_MNIST = False\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/', train=True, transform = torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\n",
    "train_loader = Data.DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist/',train = False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim = 1),volatile = True).type(torch.FloatTensor)[:500]/255.\n",
    "test_y = test_data.test_labels[:500].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c9c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 20 ===== ===== test accuracy is  0.172 ==========\n",
      "========== 40 ===== ===== test accuracy is  0.502 ==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    124\u001b[0m     alexnet \u001b[38;5;241m=\u001b[39m myAlexNet()\n\u001b[0;32m--> 125\u001b[0m     train(alexnet)\n",
      "Cell \u001b[0;32mIn[9], line 117\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(alexnet)\u001b[0m\n\u001b[1;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output,y)\n\u001b[1;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 117\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    118\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class myAlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myAlexNet, self).__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=96,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=96,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=384,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=384,\n",
    "                out_channels=384,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=384,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(keep_prob_rate),\n",
    "        )\n",
    "\n",
    "        self.fc_2 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(keep_prob_rate),\n",
    "        )\n",
    "\n",
    "        self.fc_3 = nn.Sequential(\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "    def saveXasjpg(self,x):\n",
    "        x = x.data.numpy()\n",
    "        x = np.squeeze(x)\n",
    "        x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "        x = (x * 255).astype(np.uint8)\n",
    "        for i in range(x.shape[0]):\n",
    "            img = Image.fromarray(x[i])\n",
    "            img.save(f'output_{i}.jpg')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x) # 1*28*28 -> 96*26*26\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # 96*26*26 -> 96*13*13\n",
    "        x = self.conv_2(x) # 96*13*13 -> 256*13*13\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # 256*13*13 -> 256*6*6\n",
    "        x = self.conv_3(x) # 256*6*6 -> 384*6*6\n",
    "        x = self.conv_4(x) # 384*6*6 -> 384*6*6\n",
    "        x = self.conv_5(x) # 384*6*6 -> 256*6*6\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # 256*6*6 -> 256*3*3\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "        \n",
    "def test(alexnet):\n",
    "    global prediction\n",
    "    y_pre = alexnet(test_x)\n",
    "    _,pre_index= torch.max(y_pre,1)\n",
    "    pre_index= pre_index.view(-1)\n",
    "    prediction = pre_index.data.numpy()\n",
    "    correct  = np.sum(prediction == test_y)\n",
    "    return correct / 500.0\n",
    "\n",
    "\n",
    "def train(alexnet):\n",
    "    optimizer = torch.optim.Adam(alexnet.parameters(), lr=learning_rate )\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (x_, y_) in enumerate(train_loader):\n",
    "            x ,y= Variable(x_),Variable(y_)\n",
    "            output = alexnet(x)  \n",
    "            loss = loss_func(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step != 0 and step % 20 ==0:\n",
    "                print(\"=\" * 10,step,\"=\"*5,\"=\"*5, \"test accuracy is \",test(alexnet) ,\"=\" * 10 )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    alexnet = myAlexNet()\n",
    "    train(alexnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
